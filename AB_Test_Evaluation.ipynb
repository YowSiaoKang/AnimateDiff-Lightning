{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc30ca91bef14281b6f5fa6d6268930f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>Evaluation Completed</h2>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46da75b650e548cea1d61bf9bd78cabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p>Thank you for completing the A-B test evaluation!</p>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261fba3e39294fe48ff803a472360208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Download Results', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Click here to download the results: <a href='AB_Test_Results.csv' target='_blank'>AB_Test_Results.csv</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\OneDrive\\Desktop\\CS470\\project\\Evaluate\\AB_Test_Results.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image, FileLink\n",
    "from pathlib import Path\n",
    "import random  # Imported for random assignments\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Prompts and GIF Paths\n",
    "# -------------------------------\n",
    "\n",
    "# Define paths\n",
    "PROMPTS_FILE = \"Text_Prompts.json\"\n",
    "GENERATED_GIFS_DIR = \"Generated_GIFs\"\n",
    "RESULTS_FILE = \"AB_Test_Results.csv\"\n",
    "\n",
    "# Load prompts from JSON file\n",
    "if not os.path.exists(PROMPTS_FILE):\n",
    "    print(f\"Prompts file '{PROMPTS_FILE}' not found.\")\n",
    "    evaluation_tasks = []\n",
    "else:\n",
    "    with open(PROMPTS_FILE, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Prepare list of evaluation tasks\n",
    "    evaluation_tasks = []\n",
    "\n",
    "    for theme in data.get(\"Themes\", []):\n",
    "        theme_name_original = theme.get(\"Name\", \"Unnamed_Theme\")\n",
    "        # Replace spaces with underscores for filename consistency\n",
    "        theme_name_sanitized = theme_name_original.replace(\" \", \"_\")\n",
    "        \n",
    "        for idx, prompt in enumerate(theme.get(\"Prompts\", []), 1):\n",
    "            # Incorporate model name into the filename\n",
    "            filename_a = f\"{theme_name_sanitized}_lightning_pipe_Prompt{idx}.gif\"\n",
    "            filename_b = f\"{theme_name_sanitized}_original_pipe_Prompt{idx}.gif\"\n",
    "            \n",
    "            # Construct the full paths to the GIFs\n",
    "            model_a_path = os.path.join(GENERATED_GIFS_DIR, theme_name_sanitized, \"lightning_pipe\", filename_a)\n",
    "            model_b_path = os.path.join(GENERATED_GIFS_DIR, theme_name_sanitized, \"original_pipe\", filename_b)\n",
    "            \n",
    "            # Check if both GIFs exist\n",
    "            if os.path.exists(model_a_path) and os.path.exists(model_b_path):\n",
    "                evaluation_tasks.append({\n",
    "                    \"Theme\": theme_name_original,  # Use original name for display\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"Model_A\": model_a_path,\n",
    "                    \"Model_B\": model_b_path\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Missing GIFs for '{theme_name_original}' Prompt {idx}:\")\n",
    "                if not os.path.exists(model_a_path):\n",
    "                    print(f\"  - Model A: {model_a_path}\")\n",
    "                if not os.path.exists(model_b_path):\n",
    "                    print(f\"  - Model B: {model_b_path}\")\n",
    "\n",
    "    # Debugging: Print the number of evaluation tasks\n",
    "    print(f\"Total Evaluation Tasks Loaded: {len(evaluation_tasks)}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define Evaluation Criteria\n",
    "# -------------------------------\n",
    "\n",
    "criteria = {\n",
    "        \"Video_Text_Relevance\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"],\n",
    "        \"Appearance_Distortion\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"],\n",
    "        \"Appearance_Aesthetics\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"],\n",
    "        \"Motion_Naturalness\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"],\n",
    "        \"Motion_Amplitude\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"],\n",
    "        \"Overall_Quality\": [\"Left is better\", \"Right is better\", \"Indistinguishable\"]\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create Interactive Widgets\n",
    "# -------------------------------\n",
    "\n",
    "# Initialize or load existing responses\n",
    "if os.path.exists(RESULTS_FILE):\n",
    "    df_responses = pd.read_csv(RESULTS_FILE)\n",
    "else:\n",
    "    # Initialize DataFrame with expanded columns for selections and models\n",
    "    columns = [\"Theme\", \"Prompt\"]\n",
    "    for criterion in criteria.keys():\n",
    "        columns.append(f\"{criterion}_Selection\")\n",
    "        columns.append(f\"{criterion}_Model\")\n",
    "    columns.append(\"Selected_Model\")\n",
    "    \n",
    "    df_responses = pd.DataFrame(columns=columns)\n",
    "    \n",
    "# Initialize evaluation index\n",
    "if 'current_task' not in globals():\n",
    "    current_task = 0\n",
    "\n",
    "def show_evaluation(task):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    theme = task['Theme']\n",
    "    prompt = task['Prompt']\n",
    "    model_a = task['Model_A']\n",
    "    model_b = task['Model_B']\n",
    "    \n",
    "    # Randomly assign models to left and right\n",
    "    sides = ['Left', 'Right']\n",
    "    random.shuffle(sides)\n",
    "    side_assignment = {\n",
    "        sides[0]: model_a,\n",
    "        sides[1]: model_b\n",
    "    }\n",
    "    left_model = side_assignment['Left']\n",
    "    right_model = side_assignment['Right']\n",
    "    \n",
    "    # Display Theme and Prompt\n",
    "    display(widgets.HTML(f\"<h2>Theme: {theme}</h2>\"))\n",
    "    display(widgets.HTML(f\"<h4>Prompt: {prompt}</h4>\"))\n",
    "    \n",
    "    # Display GIFs side by side with labels \"Left\" and \"Right\"\n",
    "    try:\n",
    "        with open(left_model, \"rb\") as fa:\n",
    "            gif_a = fa.read()\n",
    "        with open(right_model, \"rb\") as fb:\n",
    "            gif_b = fb.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GIFs: {e}\")\n",
    "        return\n",
    "    \n",
    "    box = widgets.HBox([\n",
    "        widgets.VBox([\n",
    "            widgets.Label(\"Left\"),\n",
    "            widgets.Image(value=gif_a, format='gif', width=300)\n",
    "        ]),\n",
    "        widgets.VBox([\n",
    "            widgets.Label(\"Right\"),\n",
    "            widgets.Image(value=gif_b, format='gif', width=300)\n",
    "        ])\n",
    "    ])\n",
    "    display(box)\n",
    "    \n",
    "    # Create radio buttons for each criterion with updated options\n",
    "    response_widgets = {}\n",
    "    for criterion, options in criteria.items():\n",
    "        radio = widgets.RadioButtons(\n",
    "            options=options,\n",
    "            description=f\"{criterion.replace('_', ' ')}:\",\n",
    "            disabled=False\n",
    "        )\n",
    "        response_widgets[criterion] = radio\n",
    "        display(radio)\n",
    "    \n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(description=\"Submit\", button_style='success')\n",
    "    display(submit_button)\n",
    "    \n",
    "    def on_submit(b):\n",
    "        global current_task, df_responses\n",
    "        responses = {}\n",
    "        models = {}\n",
    "        \n",
    "        for criterion, widget in response_widgets.items():\n",
    "            selection = widget.value\n",
    "            responses[f\"{criterion}_Selection\"] = selection\n",
    "            \n",
    "            # Map selection to model based on current assignment\n",
    "            if selection == \"Left is better\":\n",
    "                selected_model = \"A\" if left_model == task['Model_A'] else \"B\"\n",
    "            elif selection == \"Right is better\":\n",
    "                selected_model = \"A\" if right_model == task['Model_A'] else \"B\"\n",
    "            else:\n",
    "                selected_model = \"Indistinguishable\"\n",
    "            \n",
    "            models[f\"{criterion}_Model\"] = selected_model\n",
    "        \n",
    "        # Determine overall selected model based on majority\n",
    "        selection_counts = {\"A\": 0, \"B\": 0, \"Indistinguishable\": 0}\n",
    "        for model_selection in models.values():\n",
    "            selection_counts[model_selection] += 1\n",
    "        \n",
    "        if selection_counts[\"A\"] > selection_counts[\"B\"] and selection_counts[\"A\"] > selection_counts[\"Indistinguishable\"]:\n",
    "            overall_selected_model = \"A\"\n",
    "        elif selection_counts[\"B\"] > selection_counts[\"A\"] and selection_counts[\"B\"] > selection_counts[\"Indistinguishable\"]:\n",
    "            overall_selected_model = \"B\"\n",
    "        elif selection_counts[\"Indistinguishable\"] > selection_counts[\"A\"] and selection_counts[\"Indistinguishable\"] > selection_counts[\"B\"]:\n",
    "            overall_selected_model = \"Indistinguishable\"\n",
    "        else:\n",
    "            overall_selected_model = \"Indistinguishable\"\n",
    "        \n",
    "        # Prepare new response entry\n",
    "        new_entry = {\n",
    "            \"Theme\": theme,\n",
    "            \"Prompt\": prompt,\n",
    "            \"Selected_Model\": overall_selected_model\n",
    "        }\n",
    "        new_entry.update(responses)\n",
    "        new_entry.update(models)\n",
    "        \n",
    "        # Append to DataFrame\n",
    "        new_response = pd.DataFrame([new_entry])\n",
    "        df_responses = pd.concat([df_responses, new_response], ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        try:\n",
    "            df_responses.to_csv(RESULTS_FILE, index=False)\n",
    "            print(f\"Response saved for Theme: '{theme}', Prompt: '{prompt}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving response: {e}\")\n",
    "        \n",
    "        # Move to next task\n",
    "        current_task += 1\n",
    "        \n",
    "        if current_task < len(evaluation_tasks):\n",
    "            show_evaluation(evaluation_tasks[current_task])\n",
    "        else:\n",
    "            show_completion()\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "\n",
    "def show_completion():\n",
    "    clear_output(wait=True)\n",
    "    display(widgets.HTML(\"<h2>Evaluation Completed</h2>\"))\n",
    "    display(widgets.HTML(\"<p>Thank you for completing the A-B test evaluation!</p>\"))\n",
    "    \n",
    "    # Download button for results\n",
    "    download_button = widgets.Button(\n",
    "        description=\"Download Results\",\n",
    "        button_style='info'\n",
    "    )\n",
    "    display(download_button)\n",
    "    \n",
    "    def on_download(b):\n",
    "        display(FileLink(RESULTS_FILE, result_html_prefix=\"Click here to download the results: \"))\n",
    "    \n",
    "    download_button.on_click(on_download)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Start Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "if len(evaluation_tasks) == 0:\n",
    "    print(\"No evaluation tasks found. Please ensure that 'Text_Prompts.json' is populated with prompts and corresponding GIFs exist.\")\n",
    "elif current_task < len(evaluation_tasks):\n",
    "    show_evaluation(evaluation_tasks[current_task])\n",
    "else:\n",
    "    show_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
